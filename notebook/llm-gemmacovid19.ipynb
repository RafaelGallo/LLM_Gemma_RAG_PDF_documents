{"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"L4","authorship_tag":"ABX9TyNSKJ+h+Wh3IoWlS16u3lWC"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1023590,"sourceType":"datasetVersion","datasetId":563371},{"sourceId":1054114,"sourceType":"datasetVersion","datasetId":583476},{"sourceId":3756201,"sourceType":"datasetVersion","datasetId":551982},{"sourceId":26154,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":22015,"modelId":3301}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b> LLM Gemma - Covid19</b></div>\n\n<div style=\"text-align: center;\">\n  <img src=\"https://img.freepik.com/vetores-gratis/banner-do-surto-de-coronavirus-covid-19-com-celulas-virais_1017-24631.jpg?t=st=1724384629~exp=1724388229~hmac=5c31ce6c796ba544054e1e57d15f76f07b9190bbc62387b38b46c4c5355933f3&w=740\" alt=\"Banner do Surto de Coronavirus\" />\n</div>","metadata":{}},{"cell_type":"code","source":"# Installing packages\n!pip install transformers\n!pip install sentence_transformers\n!pip install faiss-cpu\n!pip install torch\n!pip install PyPDF2\n!pip install nltk","metadata":{"execution":{"iopub.status.busy":"2024-08-23T04:35:36.397076Z","iopub.execute_input":"2024-08-23T04:35:36.397508Z","iopub.status.idle":"2024-08-23T04:37:03.368123Z","shell.execute_reply.started":"2024-08-23T04:35:36.397465Z","shell.execute_reply":"2024-08-23T04:37:03.367007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing libraries\n\n# Importing system\nimport os\nimport faiss\n\n# Importing documents\nimport PyPDF2\n\n# Importing csv and math libraries\nimport numpy as np\nimport pandas as pd\n\n# Importing Libraries LLM\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom sentence_transformers import SentenceTransformer","metadata":{"id":"vaY0kWchDaar","execution":{"iopub.status.busy":"2024-08-23T04:40:05.109623Z","iopub.execute_input":"2024-08-23T04:40:05.110463Z","iopub.status.idle":"2024-08-23T04:40:05.116306Z","shell.execute_reply.started":"2024-08-23T04:40:05.110420Z","shell.execute_reply":"2024-08-23T04:40:05.115090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing libraries natural language processing \nimport nltk\nfrom nltk.tokenize import sent_tokenize\n\n# Downloading package nlp punkt\nnltk.download('punkt')","metadata":{"id":"ZJoSesUEEEJl","execution":{"iopub.status.busy":"2024-08-23T04:40:07.596538Z","iopub.execute_input":"2024-08-23T04:40:07.596935Z","iopub.status.idle":"2024-08-23T04:40:07.605237Z","shell.execute_reply.started":"2024-08-23T04:40:07.596898Z","shell.execute_reply":"2024-08-23T04:40:07.604116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Authentication with HUGGING FACE\nimport os\nHUGGING_FACE_ACCESS_TOKEN = os.environ['HUGGING_FACE_ACCESS_TOKEN'] = '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'","metadata":{"execution":{"iopub.status.busy":"2024-08-23T04:40:08.367934Z","iopub.execute_input":"2024-08-23T04:40:08.368322Z","iopub.status.idle":"2024-08-23T04:40:08.374156Z","shell.execute_reply.started":"2024-08-23T04:40:08.368286Z","shell.execute_reply":"2024-08-23T04:40:08.372978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# File path for PDF processing\npdf_path = \"/kaggle/input/acetic-acidas-antiviral-agent/US8957108.pdf\"\n\n# Initialize the DataFrame to store paths, text chunks, and embeddings\ndf_documents = pd.DataFrame(columns=['path', 'text_chunks', 'embeddings'])","metadata":{"execution":{"iopub.status.busy":"2024-08-23T04:40:09.201768Z","iopub.execute_input":"2024-08-23T04:40:09.202647Z","iopub.status.idle":"2024-08-23T04:40:09.208950Z","shell.execute_reply.started":"2024-08-23T04:40:09.202597Z","shell.execute_reply":"2024-08-23T04:40:09.207803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM\n\n# Model parameters\nmodel_name = 'google/gemma-2-2b-it'\n\n# Load the pre-trained model with specified configurations\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\n                                             torch_dtype=torch.float16,\n                                             token=HUGGING_FACE_ACCESS_TOKEN).to('cuda')\nmodel","metadata":{"id":"ubO28BzuEJ3N","execution":{"iopub.status.busy":"2024-08-23T04:40:10.025545Z","iopub.execute_input":"2024-08-23T04:40:10.025968Z","iopub.status.idle":"2024-08-23T04:40:15.967066Z","shell.execute_reply.started":"2024-08-23T04:40:10.025927Z","shell.execute_reply":"2024-08-23T04:40:15.966025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Documentation for LLM Gemma Model Implementation\n\nThis documentation outlines the steps for implementing the LLM Gemma model using the `AutoModelForCausalLM` class from the Hugging Face Transformers library. The model is deployed on a CUDA-enabled GPU to leverage hardware acceleration.\n\n#### 1. **Model Name**\n- `model_name`: The specific model used is `'google/gemma-2-2b-it'`, a pre-trained large language model by Google, designed for Italian text generation and natural language processing tasks.\n\n#### 2. **Loading the Model**\n- The model is loaded using the `AutoModelForCausalLM.from_pretrained()` method. This function retrieves the pre-trained model based on the specified model name and configures it for causal language modeling tasks.\n\n#### 3. **Torch Data Type**\n- `torch_dtype=torch.float16`: The model utilizes 16-bit floating point precision (`float16`) for faster computation and reduced memory usage on the GPU, which is particularly useful for large models.\n\n#### 4. **Token Authentication**\n- `token=HUGGING_FACE_ACCESS_TOKEN`: Access to the model requires authentication via a Hugging Face API token. This token is necessary to access the model from the Hugging Face Hub.\n\n#### 5. **Deploying to CUDA**\n- `.to('cuda')`: The model is deployed to a CUDA-enabled GPU using `.to('cuda')`. This ensures that the model operations are executed on the GPU, significantly speeding up the processing time compared to CPU execution.\n\n#### 7. **Summary**\nThis setup allows for the efficient loading and deployment of the Gemma LLM on a GPU, optimized for tasks involving Italian language processing. By using `float16` precision and deploying on CUDA, the model achieves faster inference times and better resource utilization.","metadata":{}},{"cell_type":"code","source":"# Load the tokenizer with the specified token\ntokenizer = AutoTokenizer.from_pretrained(model_name, token=HUGGING_FACE_ACCESS_TOKEN)","metadata":{"id":"NwlPvrz1FbrG","execution":{"iopub.status.busy":"2024-08-23T04:40:15.968906Z","iopub.execute_input":"2024-08-23T04:40:15.969236Z","iopub.status.idle":"2024-08-23T04:40:16.881137Z","shell.execute_reply.started":"2024-08-23T04:40:15.969201Z","shell.execute_reply":"2024-08-23T04:40:16.880192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1. **Tokenizer Initialization**\n- `tokenizer`: The tokenizer is initialized using the `AutoTokenizer.from_pretrained()` method from the Hugging Face Transformers library.\n\n#### 2. **Model Name**\n- `model_name`: The same model name `'google/gemma-2-2b-it'` is used to ensure that the tokenizer is compatible with the pre-trained model.\n\n#### 3. **Token Authentication**\n- `token=HUGGING_FACE_ACCESS_TOKEN`: Similar to the model, the tokenizer also requires access to the Hugging Face API, authenticated via an API token. This token grants access to the tokenizer associated with the specific model.\n\n#### 4. **Summary**\nThe tokenizer is a crucial component that ensures the text is correctly pre-processed and post-processed for the LLM Gemma model. By using the `AutoTokenizer` with the correct model name and authentication token, you ensure seamless integration with the pre-trained model. This setup allows for efficient text encoding and decoding, necessary for generating and understanding the modelâ€™s predictions.","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\n# Initialize the sentence encoder with the specified model\nencoder = SentenceTransformer('all-MiniLM-L6-v2')","metadata":{"id":"ZxET9yKHEFbh","execution":{"iopub.status.busy":"2024-08-23T04:40:16.882443Z","iopub.execute_input":"2024-08-23T04:40:16.882775Z","iopub.status.idle":"2024-08-23T04:40:17.523498Z","shell.execute_reply.started":"2024-08-23T04:40:16.882740Z","shell.execute_reply":"2024-08-23T04:40:17.522436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1. **Sentence Encoder Initialization**\n- `encoder`: The encoder is initialized using the `SentenceTransformer` class from the SentenceTransformers library.\n\n#### 2. **Model Name**\n- `'all-MiniLM-L6-v2'`: The specific model used is `'all-MiniLM-L6-v2'`, which is a smaller, faster version of the MiniLM model. This model is optimized for sentence embeddings, providing a good balance between performance and computational efficiency.\n\n#### 3. **Usage**\nThe encoder is used to convert sentences or texts into dense vector representations (embeddings). These embeddings capture the semantic meaning of the text and can be used for tasks such as similarity comparison, clustering, or as input features for machine learning models.\n\n#### 4. **Summary**\nThe `SentenceTransformer` model `'all-MiniLM-L6-v2'` is an efficient and effective tool for generating sentence embeddings. By initializing the encoder with this model, you can easily convert sentences into vector representations that capture their semantic content, which is useful for a wide range of NLP applications. The model balances accuracy and speed, making it suitable for both large-scale and real-time applications.","metadata":{}},{"cell_type":"code","source":"# Initialize the DataFrame\ndocuments_df = pd.DataFrame(columns=['file_path', 'text_segments', 'embeddings'])","metadata":{"execution":{"iopub.status.busy":"2024-08-23T04:42:10.915622Z","iopub.execute_input":"2024-08-23T04:42:10.916358Z","iopub.status.idle":"2024-08-23T04:42:10.922773Z","shell.execute_reply.started":"2024-08-23T04:42:10.916299Z","shell.execute_reply":"2024-08-23T04:42:10.921587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_text_from_pdf(file_path):\n    try:\n        with open(file_path, 'rb') as pdf_file:\n            pdf_reader = PyPDF2.PdfReader(pdf_file)\n            extracted_text = \"\".join([page.extract_text() for page in pdf_reader.pages])\n        return extracted_text\n    except Exception as error:\n        print(f\"Error reading {file_path}: {error}\")\n        return \"\"\n\ndef divide_text_into_segments(full_text, segment_size=1000):\n    sentence_list = sent_tokenize(full_text)\n    text_segments = []\n    current_segment = \"\"\n\n    for sentence in sentence_list:\n        if len(current_segment) + len(sentence) <= segment_size:\n            current_segment += sentence + \" \"\n        else:\n            text_segments.append(current_segment.strip())\n            current_segment = sentence + \" \"\n\n    if current_segment:\n        text_segments.append(current_segment.strip())\n\n    return text_segments","metadata":{"execution":{"iopub.status.busy":"2024-08-23T04:42:11.676954Z","iopub.execute_input":"2024-08-23T04:42:11.677405Z","iopub.status.idle":"2024-08-23T04:42:11.686506Z","shell.execute_reply.started":"2024-08-23T04:42:11.677338Z","shell.execute_reply":"2024-08-23T04:42:11.685303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the input path to the directory or file\n# Replace with your actual path\ninput_path = \"/kaggle/input/acetic-acidas-antiviral-agent/US8957108.pdf\" ","metadata":{"execution":{"iopub.status.busy":"2024-08-23T04:42:35.546687Z","iopub.execute_input":"2024-08-23T04:42:35.547779Z","iopub.status.idle":"2024-08-23T04:42:35.552058Z","shell.execute_reply.started":"2024-08-23T04:42:35.547731Z","shell.execute_reply":"2024-08-23T04:42:35.550889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to extract text from a PDF\ndef extract_text_from_pdf(pdf_path):\n    try:\n        with open(pdf_path, 'rb') as file:\n            reader = PyPDF2.PdfReader(file)\n            text = \"\".join([page.extract_text() for page in reader.pages])\n        return text\n    except Exception as e:\n        print(f\"Error reading {pdf_path}: {e}\")\n        return \"\"\n\n# Function to split text into chunks\ndef split_text_into_chunks(text, max_chunk_size=1000):\n    sentences = sent_tokenize(text)\n    chunks = []\n    current_chunk = \"\"\n\n    for sentence in sentences:\n        if len(current_chunk) + len(sentence) <= max_chunk_size:\n            current_chunk += sentence + \" \"\n        else:\n            chunks.append(current_chunk.strip())\n            current_chunk = sentence + \" \"\n\n    if current_chunk:\n        chunks.append(current_chunk.strip())\n\n    return chunks\n\n# Load the sentence transformer model\nencoder = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Directory or file path for PDF processing\npdf_directory = \"/kaggle/input/article-a-serological-assay-to-detect-sarscov2/2020.03.17.20037713v1.full.pdf\"\n\n# Initialize the DataFrame to store paths, text chunks, and embeddings\ndf_documents = pd.DataFrame(columns=['path', 'text_chunks', 'embeddings'])\n\n# Check if the path is a directory or a file\nif os.path.isdir(pdf_directory):\n    # If it's a directory, iterate over the files in it\n    for filename in os.listdir(pdf_directory):\n        if filename.endswith(\".pdf\"):\n            print(filename)\n            pdf_path = os.path.join(pdf_directory, filename)\n            text = extract_text_from_pdf(pdf_path)\n            chunks = split_text_into_chunks(text)\n            document_embeddings = encoder.encode(chunks)\n            new_row = pd.DataFrame({'path': [pdf_path], 'text_chunks': [chunks], 'embeddings': [document_embeddings]})\n            df_documents = pd.concat([df_documents, new_row], ignore_index=True)\nelif os.path.isfile(pdf_directory):\n    # If it's a file, process it directly\n    pdf_path = pdf_directory\n    print(pdf_path)\n    text = extract_text_from_pdf(pdf_path)\n    chunks = split_text_into_chunks(text)\n    document_embeddings = encoder.encode(chunks)\n    new_row = pd.DataFrame({'path': [pdf_path], 'text_chunks': [chunks], 'embeddings': [document_embeddings]})\n    df_documents = pd.concat([df_documents, new_row], ignore_index=True)\nelse:\n    # If the path is neither a directory nor a file, print an error message\n    print(f\"{pdf_directory} is neither a valid directory nor a file.\")\n\n# Display the resulting DataFrame\ndf_documents","metadata":{"execution":{"iopub.status.busy":"2024-08-23T04:42:36.327618Z","iopub.execute_input":"2024-08-23T04:42:36.328005Z","iopub.status.idle":"2024-08-23T04:42:37.849675Z","shell.execute_reply.started":"2024-08-23T04:42:36.327970Z","shell.execute_reply":"2024-08-23T04:42:37.848648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a FAISS index from all document embeddings\n\n# Stack all embeddings from the DataFrame into a single numpy array\nall_embeddings = np.vstack(df_documents['embeddings'].tolist())\n\n# Determine the dimensionality of the embeddings\ndimension = all_embeddings.shape[1]\n\n# Initialize a FAISS index with L2 (Euclidean) distance metric\nindex = faiss.IndexFlatL2(dimension)\n\n# Add all embeddings to the FAISS index\nindex.add(all_embeddings)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-23T04:42:38.943957Z","iopub.execute_input":"2024-08-23T04:42:38.944647Z","iopub.status.idle":"2024-08-23T04:42:38.950537Z","shell.execute_reply.started":"2024-08-23T04:42:38.944605Z","shell.execute_reply":"2024-08-23T04:42:38.949448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_most_similar_segments(search_query, top_k=20):\n    \"\"\"\n    Find the most similar text segments to the search query using FAISS index.\n\n    Parameters:\n    search_query (str): The query string to search for similar segments.\n    top_k (int): The number of top similar segments to retrieve. Default is 3.\n\n    Returns:\n    list: A list of dictionaries containing the document path, the text segment, and the similarity distance.\n    \"\"\"\n    query_embedding = encoder.encode([search_query])\n    distances, indices = index.search(query_embedding, top_k)\n    similar_segments = []\n    total_segments = sum(len(segments) for segments in documents_df['text_segments'])\n\n    for i, idx in enumerate(indices[0]):\n        if idx < total_segments:\n            doc_idx = 0\n            segment_idx = idx\n            while segment_idx >= len(documents_df['text_segments'].iloc[doc_idx]):\n                segment_idx -= len(documents_df['text_segments'].iloc[doc_idx])\n                doc_idx += 1\n            similar_segments.append({\n                'document': documents_df['file_path'].iloc[doc_idx],\n                'segment': documents_df['text_segments'].iloc[doc_idx][segment_idx],\n                'distance': distances[0][i]\n            })\n    return similar_segments\n\ndef generate_answer(search_query, context_text, max_length=1000):\n    \"\"\"\n    Generate an answer to a query based on the provided context using a pre-trained language model.\n\n    Parameters:\n    search_query (str): The query string to generate an answer for.\n    context_text (str): The context text to base the answer on.\n    max_length (int): The maximum length of the generated answer. Default is 1000 tokens.\n\n    Returns:\n    str: The generated answer.\n    \"\"\"\n    prompt = f\"Context: {context_text}\\n\\nQuestion: {search_query}\\n\\nAnswer:\"\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda')\n\n    with torch.no_grad():\n        output = model.generate(input_ids, max_new_tokens=max_length, num_return_sequences=1)\n\n    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n\n    # Extracting the answer part by removing the prompt portion\n    answer_start = decoded_output.find(\"Answer:\") + len(\"Answer:\")\n    generated_answer = decoded_output[answer_start:].strip()\n\n    return generated_answer\n\ndef search_documents(search_query):\n    \"\"\"\n    Search for the most relevant document segments and generate an answer to the query.\n\n    Parameters:\n    search_query (str): The query string to search for and generate a response to.\n\n    Returns:\n    tuple: A tuple containing the generated answer and the list of similar segments.\n    \"\"\"\n    similar_segments = find_most_similar_segments(search_query)\n    context_text = \" \".join([result['segment'].replace(\"\\n\", \"\") for result in similar_segments])\n    generated_answer = generate_answer(search_query, context_text)\n    return generated_answer, similar_segments\n","metadata":{"execution":{"iopub.status.busy":"2024-08-23T04:43:00.109793Z","iopub.execute_input":"2024-08-23T04:43:00.110215Z","iopub.status.idle":"2024-08-23T04:43:00.124256Z","shell.execute_reply.started":"2024-08-23T04:43:00.110177Z","shell.execute_reply":"2024-08-23T04:43:00.123027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Questions ","metadata":{}},{"cell_type":"code","source":"# Define the search query\nsearch_query = \"vaccine proteins vaccine for covid19\"\n\n# Run the search and generate an answer\ngenerated_answer, relevant_segments = search_documents(search_query)\n\n# Print the query and the generated answer\nprint(f\"Query: {search_query}\\n\\n-----\\n\")\nprint(f\"Generated answer: {generated_answer}\\n\\n-----\\n\")\n\n# Print the relevant segments\nprint(\"Relevant segments:\")\nfor segment in relevant_segments:\n    print(f\"Document: {segment['document']}\")\n    print(f\"Segment: {segment['segment']}\".replace(\"\\n\", \"\"))\n    print(f\"Distance: {segment['distance']}\")\n    print()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-23T04:45:45.125244Z","iopub.execute_input":"2024-08-23T04:45:45.125700Z","iopub.status.idle":"2024-08-23T04:46:10.765086Z","shell.execute_reply.started":"2024-08-23T04:45:45.125657Z","shell.execute_reply":"2024-08-23T04:46:10.764004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the search query\nsearch_query = \"vaccine SARS-CoV-2\"\n\n# Run the search and generate an answer\ngenerated_answer, relevant_segments = search_documents(search_query)\n\n# Print the query and the generated answer\nprint(f\"Query: {search_query}\\n\\n-----\\n\")\nprint(f\"Generated answer: {generated_answer}\\n\\n-----\\n\")\n\n# Print the relevant segments\nprint(\"Relevant segments:\")\nfor segment in relevant_segments:\n    print(f\"Document: {segment['document']}\")\n    print(f\"Segment: {segment['segment']}\".replace(\"\\n\", \"\"))\n    print(f\"Distance: {segment['distance']}\")\n    print()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-23T04:44:44.064776Z","iopub.execute_input":"2024-08-23T04:44:44.065153Z","iopub.status.idle":"2024-08-23T04:45:19.387385Z","shell.execute_reply.started":"2024-08-23T04:44:44.065112Z","shell.execute_reply":"2024-08-23T04:45:19.386138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the search query\nsearch_query = \"covid\"\n\n# Run the search and generate an answer\ngenerated_answer, relevant_segments = search_documents(search_query)\n\n# Print the query and the generated answer\nprint(f\"Query: {search_query}\\n\\n-----\\n\")\nprint(f\"Generated answer: {generated_answer}\\n\\n-----\\n\")\n\n# Print the relevant segments\nprint(\"Relevant segments:\")\nfor segment in relevant_segments:\n    print(f\"Document: {segment['document']}\")\n    print(f\"Segment: {segment['segment']}\".replace(\"\\n\", \"\"))\n    print(f\"Distance: {segment['distance']}\")\n    print()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-23T04:43:46.185064Z","iopub.execute_input":"2024-08-23T04:43:46.185563Z","iopub.status.idle":"2024-08-23T04:44:13.122404Z","shell.execute_reply.started":"2024-08-23T04:43:46.185511Z","shell.execute_reply":"2024-08-23T04:44:13.121287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the search query\nsearch_query = \"covid 19 vaccine proteins\"\n\n# Run the search and generate an answer\ngenerated_answer, relevant_segments = search_documents(search_query)\n\n# Print the query and the generated answer\nprint(f\"Query: {search_query}\\n\\n-----\\n\")\nprint(f\"Generated answer: {generated_answer}\\n\\n-----\\n\")\n\n# Print the relevant segments\nprint(\"Relevant segments:\")\nfor segment in relevant_segments:\n    print(f\"Document: {segment['document']}\")\n    print(f\"Segment: {segment['segment']}\".replace(\"\\n\", \"\"))\n    print(f\"Distance: {segment['distance']}\")\n    print()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-23T04:46:28.078590Z","iopub.execute_input":"2024-08-23T04:46:28.079025Z","iopub.status.idle":"2024-08-23T04:46:50.650848Z","shell.execute_reply.started":"2024-08-23T04:46:28.078981Z","shell.execute_reply":"2024-08-23T04:46:50.649641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reference\n\nCarraro, F. (2024). *AI RAG PDF Search in Multiple Documents using Gemma 2 2B on Colab* [GitHub repository]. \n\nGitHub. https://github.com/fabriciocarraro/AI_RAG_PDF_Search_in_multiple_documents_using_Gemma_2_2B_on_Colab","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}